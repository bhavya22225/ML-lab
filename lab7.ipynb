{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "22ce18fd-1c4d-43e8-b8b7-9b8f2bdd71c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Gurram Bhavya Reddy\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_split.py:776: UserWarning: The least populated class in y has only 2 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for Perceptron: {'tol': 0.001, 'penalty': None, 'max_iter': 1000, 'alpha': 0.0001}\n",
      "Best cross-validation score: 0.19593908629441625\n",
      "Accuracy on test data: 0.22672064777327935\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.model_selection import RandomizedSearchCV, train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Function to load dataset\n",
    "def load_dataset(file_path):\n",
    "    return pd.read_csv(file_path)\n",
    "\n",
    "# Function to split features and target\n",
    "def split_features_and_target(df, target_col=-2):\n",
    "    X = df.iloc[:, :-2]  # All columns except the last two\n",
    "    y = df.iloc[:, target_col]  # Choose one target column\n",
    "    return X, y\n",
    "\n",
    "# Function to split data into training and testing sets\n",
    "def split_train_test(X, y, test_size=0.2, random_state=42):\n",
    "    return train_test_split(X, y, test_size=test_size, random_state=random_state)\n",
    "\n",
    "# Function to perform RandomizedSearchCV on a Perceptron\n",
    "def perform_random_search(X_train, y_train):\n",
    "    perceptron = Perceptron()\n",
    "\n",
    "    # Hyperparameter grid\n",
    "    param_dist = {\n",
    "        'penalty': [None, 'l2', 'l1', 'elasticnet'],\n",
    "        'alpha': [1e-4, 1e-3, 1e-2, 1e-1, 1],\n",
    "        'max_iter': [100, 500, 1000, 2000],\n",
    "        'tol': [1e-3, 1e-4, 1e-5],\n",
    "    }\n",
    "\n",
    "    random_search = RandomizedSearchCV(perceptron, param_distributions=param_dist, n_iter=10, cv=5, random_state=42)\n",
    "    random_search.fit(X_train, y_train)\n",
    "\n",
    "    return random_search\n",
    "\n",
    "# Function to evaluate model on test data\n",
    "def evaluate_model(model, X_test, y_test):\n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    return accuracy\n",
    "\n",
    "# Main function\n",
    "def main():\n",
    "    # Load dataset\n",
    "    file_path = r\"C:\\Users\\Gurram Bhavya Reddy\\OneDrive\\Desktop\\ML lab\\10 - Java_AST.csv\"\n",
    "    df = load_dataset(file_path)\n",
    "\n",
    "    # Split features and target (choose one target column, e.g., -2 or -1)\n",
    "    X, y = split_features_and_target(df, target_col=-2)\n",
    "\n",
    "    # Split dataset into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = split_train_test(X, y)\n",
    "\n",
    "    # Perform RandomizedSearchCV on Perceptron\n",
    "    best_model = perform_random_search(X_train, y_train)\n",
    "\n",
    "    # Print best parameters and cross-validation score\n",
    "    print(f\"Best parameters for Perceptron: {best_model.best_params_}\")\n",
    "    print(f\"Best cross-validation score: {best_model.best_score_}\")\n",
    "\n",
    "    # Evaluate on unseen test data\n",
    "    test_accuracy = evaluate_model(best_model, X_test, y_test)\n",
    "    print(f\"Accuracy on test data: {test_accuracy}\")\n",
    "\n",
    "# Run the main function\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "491045b0-2dda-41f5-99d3-92517b77b0d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV, StratifiedKFold, train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Function to load dataset\n",
    "def load_dataset(file_path):\n",
    "    return pd.read_csv(file_path)\n",
    "\n",
    "# Function to split features and target\n",
    "def split_features_and_target(df, target_col=-1):\n",
    "    X = df.iloc[:, :-2]  # All columns except the last two\n",
    "    y = df.iloc[:, target_col]  # Choose one of the target columns\n",
    "    return X, y\n",
    "\n",
    "# Function to split data into training and testing sets\n",
    "def split_train_test(X, y, test_size=0.2, random_state=42):\n",
    "    return train_test_split(X, y, test_size=test_size, random_state=random_state)\n",
    "\n",
    "# Function to create the MLP pipeline\n",
    "def create_pipeline():\n",
    "    return Pipeline([\n",
    "        ('scaler', StandardScaler()),  # Feature scaling\n",
    "        ('mlp', MLPClassifier(max_iter=2000))  # Increased max_iter for better convergence\n",
    "    ])\n",
    "\n",
    "# Function to perform RandomizedSearchCV\n",
    "def perform_random_search(X_train, y_train, pipeline):\n",
    "    # Hyperparameter grid\n",
    "    param_dist = {\n",
    "        'mlp__hidden_layer_sizes': [(50,), (100,), (50, 50), (100, 50), (100, 100)],\n",
    "        'mlp__activation': ['tanh', 'relu'],\n",
    "        'mlp__solver': ['adam', 'sgd'],\n",
    "        'mlp__alpha': [1e-4, 1e-3, 1e-2],\n",
    "        'mlp__learning_rate': ['constant', 'adaptive'],\n",
    "    }\n",
    "\n",
    "    # StratifiedKFold for better class distribution in CV\n",
    "    cv = StratifiedKFold(n_splits=5)\n",
    "\n",
    "    # RandomizedSearchCV for MLP\n",
    "    random_search = RandomizedSearchCV(pipeline, param_distributions=param_dist, n_iter=10, cv=cv, random_state=42)\n",
    "    random_search.fit(X_train, y_train)\n",
    "\n",
    "    return random_search\n",
    "\n",
    "# Function to evaluate model on test data\n",
    "def evaluate_model(model, X_test, y_test):\n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    return accuracy\n",
    "\n",
    "# Main function\n",
    "def main():\n",
    "    # Load dataset\n",
    "    file_path = r\"C:\\Users\\Gurram Bhavya Reddy\\OneDrive\\Desktop\\ML lab\\10 - Java_AST.csv\"\n",
    "    df = load_dataset(file_path)\n",
    "\n",
    "    # Split features and target (choose one target column, e.g., -2 or -1)\n",
    "    X, y = split_features_and_target(df, target_col=-1)\n",
    "\n",
    "    # Split dataset into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = split_train_test(X, y)\n",
    "\n",
    "    # Create MLP pipeline\n",
    "    pipeline = create_pipeline()\n",
    "\n",
    "    # Perform RandomizedSearchCV on MLP\n",
    "    best_model = perform_random_search(X_train, y_train, pipeline)\n",
    "\n",
    "    # Print best parameters and cross-validation score\n",
    "    print(f\"Best parameters for MLP: {best_model.best_params_}\")\n",
    "    print(f\"Best cross-validation score: {best_model.best_score_}\")\n",
    "\n",
    "    # Evaluate on unseen test data\n",
    "    test_accuracy = evaluate_model(best_model, X_test, y_test)\n",
    "    print(f\"Accuracy on test data: {test_accuracy}\")\n",
    "\n",
    "# Run the main function\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68bdadb2-7ccb-4fbb-83a6-e6e53b176082",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install catboost xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c32727b0-84ac-448b-8489-890a0a8272f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# Function to load the dataset and remap target class labels\n",
    "def load_and_prepare_data(file_path):\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Step 1: Sequentially remap the target class labels\n",
    "    unique_classes = df.iloc[:, -1].unique()  # Assuming the last column is the target column\n",
    "    class_mapping = {old_label: new_label for new_label, old_label in enumerate(sorted(unique_classes))}\n",
    "    df.iloc[:, -1] = df.iloc[:, -1].map(class_mapping)\n",
    "    \n",
    "    print(\"Mapped Class Labels:\", df.iloc[:, -1].unique())\n",
    "    \n",
    "    # Step 2: Split features (X) and target (y)\n",
    "    X = df.iloc[:, :-2]  # All columns except the last two\n",
    "    y = df.iloc[:, -1]  # The remapped target column\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "# Function to split dataset into train and test sets\n",
    "def split_data(X, y, test_size=0.2, random_state=42):\n",
    "    return train_test_split(X, y, test_size=test_size, random_state=random_state)\n",
    "\n",
    "# Function to scale the data\n",
    "def scale_data(X_train, X_test):\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    return X_train_scaled, X_test_scaled\n",
    "\n",
    "# Function to define classifiers\n",
    "def get_classifiers():\n",
    "    return {\n",
    "        'SVM': SVC(probability=True, random_state=42),\n",
    "        'Decision Tree': DecisionTreeClassifier(random_state=42),\n",
    "        'Random Forest': RandomForestClassifier(random_state=42),\n",
    "        'AdaBoost': AdaBoostClassifier(random_state=42),\n",
    "        'XGBoost': XGBClassifier(random_state=42),\n",
    "        'CatBoost': CatBoostClassifier(verbose=0, random_state=42),\n",
    "        'Naive Bayes': GaussianNB()\n",
    "    }\n",
    "\n",
    "# Function to evaluate a classifier and return the performance metrics\n",
    "def evaluate_classifier(clf, clf_name, X_train_scaled, X_test_scaled, y_train, y_test):\n",
    "    print(f\"Training {clf_name}...\")\n",
    "    clf.fit(X_train_scaled, y_train)\n",
    "    y_pred = clf.predict(X_test_scaled)\n",
    "\n",
    "    # Predict probabilities for ROC AUC\n",
    "    if hasattr(clf, 'predict_proba'):\n",
    "        if len(y_train.unique()) == 2:  # Binary classification\n",
    "            y_prob = clf.predict_proba(X_test_scaled)[:, 1]\n",
    "        else:  # Multiclass classification\n",
    "            y_prob = clf.predict_proba(X_test_scaled)\n",
    "    else:\n",
    "        y_prob = None\n",
    "\n",
    "    # Calculate performance metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred, average='weighted')\n",
    "    recall = recall_score(y_test, y_pred, average='weighted')\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "    \n",
    "    # Handle ROC AUC for binary and multiclass cases\n",
    "    if y_prob is not None:\n",
    "        if len(y_train.unique()) == 2:\n",
    "            roc_auc = roc_auc_score(y_test, y_prob)\n",
    "        else:\n",
    "            roc_auc = roc_auc_score(y_test, y_prob, multi_class='ovr')\n",
    "    else:\n",
    "        roc_auc = None\n",
    "    \n",
    "    return {\n",
    "        'Classifier': clf_name,\n",
    "        'Accuracy': accuracy,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1 Score': f1,\n",
    "        'ROC AUC': roc_auc\n",
    "    }\n",
    "\n",
    "# Main function to run the classification and evaluation\n",
    "def main():\n",
    "    # Load the dataset and prepare data\n",
    "    file_path = r\"C:\\Users\\Gurram Bhavya Reddy\\OneDrive\\Desktop\\ML lab\\10 - Java_AST.csv\"\n",
    "    X, y = load_and_prepare_data(file_path)\n",
    "    \n",
    "    # Split the data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = split_data(X, y)\n",
    "    \n",
    "    # Scale the features\n",
    "    X_train_scaled, X_test_scaled = scale_data(X_train, X_test)\n",
    "    \n",
    "    # Get the classifiers\n",
    "    classifiers = get_classifiers()\n",
    "    \n",
    "    # Store results\n",
    "    results = []\n",
    "    \n",
    "    # Evaluate each classifier\n",
    "    for clf_name, clf in classifiers.items():\n",
    "        result = evaluate_classifier(clf, clf_name, X_train_scaled, X_test_scaled, y_train, y_test)\n",
    "        results.append(result)\n",
    "    \n",
    "    # Convert results to a DataFrame and display\n",
    "    results_df = pd.DataFrame(results)\n",
    "    print(results_df)\n",
    "\n",
    "# Run the main function\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86fb1e96-b462-4244-8105-815e1d4403ea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
